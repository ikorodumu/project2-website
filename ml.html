<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Teachable Skin ‚Äì Machine Learning Project</title>
  <link rel="stylesheet" href="stylepage.css" />
  <script defer src="script.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest/dist/tf.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@teachablemachine/image@latest/dist/teachablemachine-image.min.js"></script>
</head>
<body>

  <!-- ÎÑ§ÎπÑÍ≤åÏù¥ÏÖò Î∞î -->
  <nav>
    <ul>
      <li><a href="index.html">Home</a></li>
      <li><a href="about.html">About Us</a></li>
      <li><a href="resources.html">Resources</a></li>
      <li><a href="tech-hero.html">Tech Hero</a></li>
      <li><a href="ml.html">ML Project</a></li>
    </ul>
  </nav>

  <!-- ÌéòÏù¥ÏßÄ Ìó§Îçî -->
  <header class="hero-header">
    <h1>Teachable Skin: Exploring Algorithmic Bias</h1>
    <p>Our machine learning experiment using Google's Teachable Machine</p>
  </header>

  <!-- ÏÑúÎ∏åÌÉ≠ Î©îÎâ¥ -->
  <div class="tab-container">
    <ul class="inner-tabs">
      <li><a href="#" class="inner-tab-link active" data-tab="demo">Model Demo</a></li>
      <li><a href="#" class="inner-tab-link" data-tab="statement">Statement</a></li>
      <li><a href="#" class="inner-tab-link" data-tab="reflection">Extended Reflection</a></li>
      <li><a href="#" class="inner-tab-link" data-tab="challenges">Challenges</a></li>
    </ul>
  </div>

  <!-- ÌÉ≠ 1: Î™®Îç∏ Îç∞Î™® -->
  <div id="demo" class="inner-tab-content active">
    <section class="video-container">
      <h2>üé• Model Demonstration</h2>
      <iframe width="560" height="315"
              src="https://www.youtube.com/embed/3Z7swx6gicw"
              frameborder="0"
              allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
              allowfullscreen>
      </iframe>
    </section>

    <section class="video-container">
      <h2>üß† Teachable Machine Classifier</h2>
      <!-- üîΩ Ïó¨Í∏∞Ïóê TM Î™®Îç∏ embed ÎßÅÌÅ¨ ÎÑ£Ïñ¥Ï£ºÏÑ∏Ïöî -->
      <iframe src="Ïó¨Í∏∞Ïóê_TeachableMachine_embed_URL_ÎÑ£Í∏∞" 
              width="100%" 
              height="500px" 
              frameborder="0" 
              allow="camera; microphone" 
              allowfullscreen>
      </iframe>
    </section>
  </div>

  <h3>Try Our Model</h3>
<div id="model-container">
  <p>Hold your face in the camera</p>
  <button type="button" class="webcam-btn" onclick="init()">Start Webcam</button>
  <div id="webcam-container"></div>
  <div id="label-container"></div>
</div>

  <!-- ÌÉ≠ 2: Statement -->
  <div id="statement" class="inner-tab-content">
    <section class="content">
      <h2>üìù Project Statement</h2>
      <p>
        Our group decided to create an image-based classification algorithm using Google‚Äôs Teachable Machines to detect and categorize different human skin tones. This project was inspired by Joy Buolamwini‚Äôs <em>Unmasking AI</em>, which powerfully critiques how artificial intelligence, particularly facial recognition systems, often fail to fairly recognize or represent darker-skinned individuals due to biased training data and non-inclusive development practices.
      </p>
      <p>
        The goal of our project is not to build a perfect skin tone classifier, nor do we believe AI should ever be used to label people by race or physical traits in high-stakes settings. Rather, our aim is to engage critically with machine learning technology as students and examine firsthand how easily algorithmic bias can emerge, even in beginner-friendly platforms like Teachable Machines. Our experiment‚Äî<strong>Teachable Skin</strong>‚Äîinvites deeper reflection on the ethical concerns Buolamwini raises and the dangers of ignoring representation in AI systems.
      </p>
      <p>
        We began by building a small dataset of facial images from diverse volunteers, including friends and peers with a wide range of skin tones. We organized our classes into four broad categories based on Google's very own Monk Skin Tone (MST) scale: light (MST 1-3), medium skin tone (MST 4-6), medium-dark (MST 7-8), and dark (MST 9-10). Our goal was to include at least 10 unique images within each class, all captured in well-lit, neutral conditions to minimize shadows or environmental interference.
      </p>
      <p>
        Even at this stage, we encountered challenges that prompted meaningful conversation. How do we determine where one skin tone category ends and another begins? What lighting conditions are ‚Äúneutral‚Äù? Is there any way to define skin tone in a machine-readable way without reducing people to data points? These questions reminded us that classification, even when well-intentioned, always involves subjective choices.
      </p>
      <p>
        Using the Teachable Machines‚Äò Image Project tool, we uploaded and labeled our images by skin tone group. The platform simplified training; we only needed to click a few buttons and wait while the system processed our images and generated a model.
      </p>
      <p>
        Initially, the classifier appeared to perform well, especially when tested with Muiz being the test subject. However, the accuracy declined when we introduced new test subjects with different lighting. The model often misclassified darker skin tones, sometimes labeling them as lighter or medium-dark, depending on shadows, brightness, and facial orientation.
      </p>
    </section>
  </div>


  <!-- ÌÉ≠ 3: Extended Reflection -->
  <div id="reflection" class="inner-tab-content">
    <section class="content">
      <h2>üîç Extended Reflection</h2>
      <p>
        This result closely mirrors Buolamwini‚Äôs findings in <em>Unmasking AI</em>, where facial recognition software struggled most with accurately identifying darker-skinned individuals, especially women. It was clear that despite our intention to create a balanced dataset, external factors (like lighting and camera bias) still shaped how the model interpreted skin color.
      </p>
      <p>
        Throughout this process, Buolamwini‚Äôs framework of the ‚Äúcoded gaze‚Äù helped us make sense of our results. Her argument is that AI systems are never neutral‚Äîthey reflect the assumptions, values, and limitations of those who design and train them. Even though we approached this project with a conscious effort to be inclusive, we still saw the classifier perform unevenly across different skin tones. This showed us that bias is not only about who is excluded, but also how easily human complexity gets flattened into rigid categories.
      </p>
      <p>
        Buolamwini also critiques the myth of objectivity in AI. Her research reveals how many commercial AI systems are marketed as impartial, yet consistently fail in ways that disadvantage marginalized groups. Our project, though much smaller in scale, demonstrated this phenomenon. A model trained on limited data‚Äîespecially without rigorous testing across diverse conditions‚Äîwill not generalize well, and may replicate existing social inequalities.
      </p>
      <p>
        We also reflected on Buolamwini‚Äôs emphasis on the need to participate in shaping AI. Most machine learning systems today are developed by relatively homogeneous teams, often without meaningful input from the communities most affected by these technologies. Our group discussed how this dynamic applies even to a classroom setting. If we, as students, aren‚Äôt trained to think critically about bias and representation in tech, we risk contributing to these problems in our future careers.
      </p>
      <p>
        Some technical limitations we encountered included lighting sensitivity, small sample size, and subjectivity in the chosen labels.
      </p>
      <p>
        We also considered the ethical implications of building a skin tone classifier. While our goal was educational, we understand that skin tone classification can be weaponized when used in surveillance, hiring, or criminal justice.
      </p>
      <p>
        This project helped us realize how accessible AI tools have become and how dangerous that accessibility can be if not accompanied by critical thinking. Anyone with a computer can build a classifier in less than an hour. What‚Äôs much harder is understanding the implications of what you‚Äôve built, who it impacts, and how it may fail.
      </p>
      <p>
        Buolamwini argues that AI systems often ‚Äúfail the rest of us‚Äù because they are built to serve the few. Our group agrees. Without diverse representation, transparent practices, and ethical accountability, machine learning systems will continue reinforcing injustice, even when they appear innovative or impressive.
      </p>
      <p>
        We leave this project with a profound respect for algorithmic accountability advocates‚Äô work and a stronger sense of responsibility as students learn to work with data. Building AI isn‚Äôt just about teaching machines, it‚Äôs about teaching ourselves to ask better, harder questions.
      </p>
    </section>
  </div>

  <!-- ÌÉ≠ 4: Challenges -->
  <div id="challenges" class="inner-tab-content">
    <section class="content">
      <h2>Challenges & Ethical Questions</h2>
      <ul style="text-align: left;">
        <li>Lighting consistency across skin tones</li>
        <li>Small sample size and risk of overfitting</li>
        <li>Subjectivity in Monk Skin Tone label assignments</li>
        <li>Ethical concerns of skin tone classification in surveillance or hiring</li>
        <li>Webcam quality and facial orientation influencing accuracy</li>
      </ul>
    </section>
  </div>

  <!-- Ìë∏ÌÑ∞ -->
  <footer>
    <p>¬© 2025 Our Website | Muiz Aminu, Noa Levine, Hari Kang</p>
  </footer>

</body>
</html>
