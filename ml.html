<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Teachable Skin – Machine Learning Project</title>
  <link rel="stylesheet" href="stylepage.css" />
</head>
<body>
 
  <!-- 네비가이션 바 -->
  <nav>
    <ul>
      <li><a href="index.html">Home</a></li>
      <li><a href="about.html">About Us</a></li>
      <li><a href="resources.html">Resources</a></li>
      <li><a href="tech-hero.html">Tech Hero</a></li>
      <li><a href="ml.html">ML Project</a></li>
    </ul>
  </nav>

  <!-- 페이지 헤더 -->
  <header class="hero-header">
    <h1>Teachable Skin: Exploring Algorithmic Bias</h1>
    <p>Our machine learning experiment using Google's Teachable Machine</p>
  </header>

  <!-- 서브탭 메뉴 -->
  <div class="tab-container">
    <ul class="inner-tabs">
      <li><a href="#" class="inner-tab-link" data-tab="statement">Statement</a></li>
      <li><a href="#" class="inner-tab-link" data-tab="reflection">Extended Reflection</a></li>
      <li><a href="#" class="inner-tab-link active" data-tab="demo">Model Demo</a></li>
      <li><a href="#" class="inner-tab-link" data-tab="challenges">Challenges</a></li>
    </ul>
  </div>

  <!-- 탭 1: 메들 데모 -->
  <div id="demo" class="inner-tab-content">
    <section class="video-container">
      <h2>Model Demonstration</h2>
      <iframe width="560" height="315"
              src="https://www.youtube.com/embed/3Z7swx6gicw"
              frameborder="0"
              allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
              allowfullscreen>
      </iframe>
    </section>

    <section class="video-container">
      <h2>Teachable Machine</h2>
      <div>Skin Tone Classifier</div>
      <button type="button" onclick="init()" class="custom-btn2">Start</button>
      <div id="webcam-container"></div>
      <div id="label-container"></div>
    </section>
  </div>

  <!-- 탭 2: Statement -->
  <div id="statement" class="inner-tab-content active">
    <section class="content">
      <h2>Project Statement</h2>
      <p>
        Our group decided to create an image-based classification algorithm using Google’s Teachable Machines to detect and categorize different human skin tones. This project was inspired by Joy Buolamwini’s <em>Unmasking AI</em>, which powerfully critiques how artificial intelligence, particularly facial recognition systems, often fail to fairly recognize or represent darker-skinned individuals due to biased training data and non-inclusive development practices.
      </p>
      <p>
        The goal of our project is not to build a perfect skin tone classifier, nor do we believe AI should ever be used to label people by race or physical traits in high-stakes settings. Rather, our aim is to engage critically with machine learning technology as students and examine firsthand how easily algorithmic bias can emerge, even in beginner-friendly platforms like Teachable Machines. Our experiment—<strong>Teachable Skin</strong>—invites deeper reflection on the ethical concerns Buolamwini raises and the dangers of ignoring representation in AI systems.
      </p>
      <p>
        We began by building a small dataset of facial images from diverse volunteers, including friends and peers with a wide range of skin tones. We organized our classes into four broad categories based on Google's very own Monk Skin Tone (MST) scale: light (MST 1-3), medium skin tone (MST 4-6), medium-dark (MST 7-8), and dark (MST 9-10). Our goal was to include at least 10 unique images within each class, all captured in well-lit, neutral conditions to minimize shadows or environmental interference.
      </p>
      <p>
        Even at this stage, we encountered challenges that prompted meaningful conversation. How do we determine where one skin tone category ends and another begins? What lighting conditions are “neutral”? Is there any way to define skin tone in a machine-readable way without reducing people to data points? These questions reminded us that classification, even when well-intentioned, always involves subjective choices.
      </p>
      <p>
        Using the Teachable Machines‘ Image Project tool, we uploaded and labeled our images by skin tone group. The platform simplified training; we only needed to click a few buttons and wait while the system processed our images and generated a model.
      </p>
      <p>
        Initially, the classifier appeared to perform well, especially when tested with Muiz being the test subject. However, the accuracy declined when we introduced new test subjects with different lighting. The model often misclassified darker skin tones, sometimes labeling them as lighter or medium-dark, depending on shadows, brightness, and facial orientation.
      </p>
    </section>
  </div>

  <!-- 탭 3: Extended Reflection -->
  <div id="reflection" class="inner-tab-content">
    <section class="content">
      <h2>Extended Reflection</h2>
      <p>
        This result closely mirrors Buolamwini’s findings in <em>Unmasking AI</em>, where facial recognition software struggled most with accurately identifying darker-skinned individuals, especially women. It was clear that despite our intention to create a balanced dataset, external factors (like lighting and camera bias) still shaped how the model interpreted skin color.
      </p>
      <p>
        Throughout this process, Buolamwini’s framework of the “coded gaze” helped us make sense of our results. Her argument is that AI systems are never neutral, they reflect the assumptions, values, and limitations of those who design and train them. Even though we approached this project with a conscious effort to be inclusive, we still saw the classifier perform unevenly across different skin tones. This showed us that bias is not only about who is excluded, but also how easily human complexity gets flattened into rigid categories.
      </p>
      <p>
        Buolamwini also critiques the myth of objectivity in AI. Her research reveals how many commercial AI systems are marketed as impartial, yet consistently fail in ways that disadvantage marginalized groups. Our project, though much smaller in scale, demonstrated this phenomenon. A model trained on limited data, especially without rigorous testing across diverse conditions, will not generalize well, and may replicate existing social inequalities.
      </p>
      <p>
        We also reflected on Buolamwini’s emphasis on the need to participate in shaping AI. Most machine learning systems today are developed by relatively homogeneous teams, often without meaningful input from the communities most affected by these technologies. Our group discussed how this dynamic applies even to a classroom setting. If we, as students, aren’t trained to think critically about bias and representation in tech, we risk contributing to these problems in our future careers.
      </p>
      <p>
        Some technical limitations we encountered included lighting sensitivity, small sample size, and subjectivity in the chosen labels.
      </p>
      <p>
        We also considered the ethical implications of building a skin tone classifier. While our goal was educational, we understand that skin tone classification can be weaponized when used in surveillance, hiring, or criminal justice.
      </p>
      <p>
        This project helped us realize how accessible AI tools have become and how dangerous that accessibility can be if not accompanied by critical thinking. Anyone with a computer can build a classifier in less than an hour. What’s much harder is understanding the implications of what you’ve built, who it impacts, and how it may fail.
      </p>
      <p>
        Buolamwini argues that AI systems often “fail the rest of us” because they are built to serve the few. Our group agrees. Without diverse representation, transparent practices, and ethical accountability, machine learning systems will continue reinforcing injustice, even when they appear innovative or impressive.
      </p>
      <p>
        We leave this project with a profound respect for algorithmic accountability advocates’ work and a stronger sense of responsibility as students learn to work with data. Building AI isn’t just about teaching machines, it’s about teaching ourselves to ask better, harder questions.
      </p>
    </section>
  </div>

  <!-- 탭 4: Challenges -->
  <div id="challenges" class="inner-tab-content">
    <section class="content">
      <h2>Challenges & Ethical Questions</h2>
      <ul style="text-align: left;">
        <li>Lighting consistency across skin tones</li>
        <li>Small sample size and risk of overfitting</li>
        <li>Subjectivity in Monk Skin Tone label assignments</li>
        <li>Ethical concerns of skin tone classification in surveillance or hiring</li>
        <li>Webcam quality and facial orientation influencing accuracy</li>
      </ul>
    </section>
  </div>

  <!-- 푸터 -->
  <footer>
    <p>© 2025 Our Website | Muiz Aminu, Noa Levine, Hari Kang</p>
  </footer>

<!-- Scripts for model functionality -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.9.0/dist/tf.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@teachablemachine/image@0.8.4/dist/teachablemachine-image.min.js"></script>
<script>
  const URL = "https://teachablemachine.withgoogle.com/models/ndVbPy1ak/";
  let model, webcam, labelContainer, maxPredictions;

  async function init() {
    const modelURL = URL + "model.json";
    const metadataURL = URL + "metadata.json";

    try {
      model = await tmImage.load(modelURL, metadataURL);
      maxPredictions = model.getTotalClasses();

      const flip = true;
      webcam = new tmImage.Webcam(200, 200, flip);
      await webcam.setup();
      await webcam.play();
      window.requestAnimationFrame(loop);

      document.getElementById("webcam-container").appendChild(webcam.canvas);
      labelContainer = document.getElementById("label-container");
      labelContainer.innerHTML = "";
      for (let i = 0; i < maxPredictions; i++) {
        labelContainer.appendChild(document.createElement("div"));
      }
    } catch (error) {
      console.error("Webcam or model loading failed:", error);
      alert("Could not start model. Check if camera permissions are allowed and you're using HTTPS.");
    }
  }

  async function loop() {
    webcam.update();
    await predict();
    window.requestAnimationFrame(loop);
  }

  async function predict() {
    const prediction = await model.predict(webcam.canvas);
    for (let i = 0; i < maxPredictions; i++) {
      const classPrediction =
        prediction[i].className + ": " + prediction[i].probability.toFixed(2);
      labelContainer.childNodes[i].innerHTML = classPrediction;
    }
  }
</script>

<script>
  // Handle tab switching
  const tabLinks = document.querySelectorAll('.inner-tab-link');
  const tabContents = document.querySelectorAll('.inner-tab-content');

  tabLinks.forEach(link => {
    link.addEventListener('click', function(e) {
      e.preventDefault();

      // Remove 'active' class from all links and contents
      tabLinks.forEach(link => link.classList.remove('active'));
      tabContents.forEach(content => content.classList.remove('active'));

      // 'active' class to the clicked tab and corresponding content
      this.classList.add('active');
      const target = this.getAttribute('data-tab');
      const activeTab = document.getElementById(target);
      if (activeTab) {
        activeTab.classList.add('active');
      }
    });
  });
</script>




</body>
</html>
